{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ricardino/Documents/MAIA/tercer_semestre/CAD/Projecte/Machine_Learning/notebooks\n"
     ]
    }
   ],
   "source": [
    "features_path = Path.cwd()\n",
    "notebooks_path = features_path.parent\n",
    "repo_path = notebooks_path.parent\n",
    "os.chdir(str(features_path))\n",
    "#print current working directory\n",
    "print(notebooks_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class path_label():\n",
    "    \"\"\"Class to access paths and labels from csv\n",
    "    \"\"\"\n",
    "    def __init__(self, meta=pd.read_csv(str(repo_path) + '/data/meta_info.csv', sep='\\t'), classif='binary', set_name='train') -> None:\n",
    "        meta = meta.loc[meta['classif'] == classif] #Filter by classif\n",
    "        meta = meta.loc[meta['set'] == set_name] #Filter by set\n",
    "        self.paths = list(meta.path)\n",
    "        self. labels = np.array(meta.label)\n",
    "        self.FOV_x1 = np.array(meta.FOV_x1, dtype=np.int16)\n",
    "        self.FOV_x2 = np.array(meta.FOV_x2, dtype=np.int16)\n",
    "        self.FOV_y1 = np.array(meta.FOV_y1, dtype=np.int16)\n",
    "        self.FOV_y2 = np.array(meta.FOV_y2, dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(file, filename):\n",
    "    \"\"\"save as pickle\n",
    "\n",
    "    Args:\n",
    "        file (obj): object to save\n",
    "        filename (str): path of the object to save\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(file, handle, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def open_pickle(filename):\n",
    "    \"\"\"open pickle file\n",
    "\n",
    "    Args:\n",
    "        filename (str): path of pickle file\n",
    "\n",
    "    Returns:\n",
    "        obj: object extracted form pickle\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as handle:\n",
    "        return pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(method, grid_type='normal', scaler=StandardScaler(), reductor_components = [0.95,0.99], verbose=4):\n",
    "    \"\"\"get pipe and grid for classifier\n",
    "\n",
    "    Args:\n",
    "        method (str): classifier name\n",
    "        scaler (scikit object, optional): type of scaler. Defaults to StandardScaler().\n",
    "        verbose (int, optional): verbose level. Defaults to 4.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "   #KNN\n",
    "    if(method=='KNN'):\n",
    "        param_grid = {'classifier__n_neighbors': list(range(1,40))}\n",
    "        pipe = Pipeline([('scaler', scaler),('classifier',KNeighborsClassifier())])\n",
    "        grid = GridSearchCV(pipe, param_grid, verbose = verbose)\n",
    "        return grid, pipe\n",
    "    if(method=='KNN_PCA'):\n",
    "        param_grid = {'reductor__n_components': reductor_components, 'classifier__n_neighbors': list(range(1,40))}\n",
    "        pipe = Pipeline([('scaler', scaler),('reductor',PCA()), ('classifier',KNeighborsClassifier())])\n",
    "        grid = GridSearchCV(pipe, param_grid, verbose = verbose)\n",
    "        return grid, pipe\n",
    "    #RF\n",
    "    elif(method=='RF'):\n",
    "        param_grid = {'classifier__n_estimators': [100, 200, 400],}\n",
    "        pipe = Pipeline([('scaler', scaler),('classifier',RandomForestClassifier())])\n",
    "        grid = GridSearchCV(pipe, param_grid, verbose = verbose)\n",
    "        return grid, pipe\n",
    "    \n",
    "    elif(method=='RF_PCA'):\n",
    "        param_grid = {'reductor__n_components': reductor_components,'classifier__n_estimators': [100, 200, 400],}\n",
    "        pipe = Pipeline([('scaler', scaler),('reductor',PCA()), ('classifier',RandomForestClassifier())])\n",
    "        grid = GridSearchCV(pipe, param_grid, scoring='accuracy', verbose = verbose)\n",
    "        return grid, pipe\n",
    "    #SVM\n",
    "    elif(method=='SVM'):\n",
    "        if(grid_type=='normal'):\n",
    "            param_grid = {'classifier__C': [0.1, 1, 10, 100],\n",
    "                    'classifier__gamma': [100, 10, 1, 0.1, 0.01, 0.001, 0.0001],\n",
    "                    'classifier__kernel': ['rbf'],'classifier__class_weight':['balanced']}\n",
    "            pipe = Pipeline([('scaler', scaler),('classifier',SVC())])\n",
    "            grid = GridSearchCV(pipe, param_grid, scoring='accuracy', verbose = verbose)\n",
    "            return grid, pipe\n",
    "        elif(grid_type=='random'):\n",
    "            parameters = {'classifier__C': scipy.stats.expon(scale=10), 'classifier__gamma': scipy.stats.expon(scale=.001), #Parameters for grid search\n",
    "            'classifier__kernel': ['rbf'], 'classifier__class_weight':['balanced']}\n",
    "            pipe = Pipeline([('scaler', scaler),('classifier',SVC())]) #Definition of pipeline\n",
    "            grid = RandomizedSearchCV(pipe, parameters,n_iter=20, scoring='accuracy', verbose=verbose, return_train_score=False) #Random search\n",
    "            return grid, pipe\n",
    "    elif(method=='SVM_PCA'):\n",
    "        if(grid_type=='normal'):\n",
    "            param_grid = {'reductor__n_components': reductor_components, 'classifier__C': [0.1, 1, 10, 100],\n",
    "                    'classifier__gamma': [100, 10, 1, 0.1, 0.01, 0.001, 0.0001],\n",
    "                    'classifier__kernel': ['rbf'],'classifier__class_weight':['balanced']}\n",
    "            pipe = Pipeline([('scaler', scaler),('reductor',PCA()), ('classifier',SVC())])\n",
    "            grid = GridSearchCV(pipe, param_grid, scoring='accuracy', verbose = verbose)\n",
    "            return grid, pipe\n",
    "        elif(grid_type=='random'):\n",
    "            parameters = {'reductor__n_components': reductor_components, 'classifier__C': scipy.stats.expon(scale=10), 'classifier__gamma': scipy.stats.expon(scale=.001), #Parameters for grid search\n",
    "            'classifier__kernel': ['rbf'], 'classifier__class_weight':['balanced']}\n",
    "            pipe = Pipeline([('scaler', scaler),('reductor',PCA()), ('classifier',SVC())]) #Definition of pipeline\n",
    "            grid = RandomizedSearchCV(pipe, parameters,n_iter=100, scoring='accuracy', verbose=verbose, return_train_score=False) #Random search\n",
    "            return grid, pipe\n",
    "    #ADA\n",
    "    elif(method=='AdaBoost'):\n",
    "        param_grid = {'classifier__n_estimators': [50,100, 200, 400], 'classifier__learning_rate': [ 0.1, 1,5, 10]}\n",
    "        pipe = Pipeline([('scaler', scaler),('classifier',AdaBoostClassifier())])\n",
    "        grid = GridSearchCV(pipe, param_grid, verbose = verbose)\n",
    "        return grid, pipe\n",
    "    elif(method=='AdaBoost_PCA'):\n",
    "        param_grid = {'reductor__n_components': reductor_components, 'classifier__n_estimators': [50,100, 200, 400], 'classifier__learning_rate': [ 0.1, 1,5, 10]}\n",
    "        pipe = Pipeline([('scaler', scaler),('reductor',PCA()),('classifier',AdaBoostClassifier())])\n",
    "        grid = GridSearchCV(pipe, param_grid, verbose = verbose)\n",
    "        return grid, pipe\n",
    "    #GradientBoosting\n",
    "    elif(method=='GradBoost'):\n",
    "        param_grid = {'classifier__learning_rate': [0.01, 0.1, 1, 10], 'classifier__max_iter': [100, 200, 400]}\n",
    "        pipe = Pipeline([('scaler', scaler), ('classifier',GradientBoostingClassifier())])\n",
    "        grid = GridSearchCV(pipe, param_grid, verbose = verbose)\n",
    "        return grid, pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_data_load(classif, color_space, f_name):\n",
    "    # Read train and valiadation data\n",
    "    meta = pd.read_csv(str(repo_path) + '/data/meta_info.csv', sep='\\t') #For labels\n",
    "    f_type = 'color'\n",
    "\n",
    "    #We read the training data\n",
    "    X_train = open_pickle(str(repo_path)+ f'/data/features/{f_type}/{color_space}/{classif}_train_{f_type}_{f_name}_fv.p')\n",
    "    y_train = path_label(meta, classif, set_name='train').labels\n",
    "    #Print shapes to be sure that dimensions are the same\n",
    "    print(f'The shape of the training data is {X_train.shape} and its labels are {y_train.shape}')\n",
    "\n",
    "    #We read the validation data\n",
    "    X_val = open_pickle(str(repo_path)+ f'/data/features/{f_type}/{color_space}/{classif}_val_{f_type}_{f_name}_fv.p')\n",
    "    y_val = path_label(meta, classif, set_name='val').labels\n",
    "    print(f'The shape of the valdiation data is {X_val.shape} and its labels are {y_val.shape}')\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texture_data_load(classif, f_name):\n",
    "    # Read train and valiadation data\n",
    "    meta = pd.read_csv(str(repo_path) + '/data/meta_info.csv', sep='\\t') #For labels\n",
    "    f_type = 'texture'\n",
    "    X_train = open_pickle(str(repo_path) + f'/data/features/{f_type}/{classif}_train_{f_type}_{f_name}_fv.p')\n",
    "    y_train = path_label(meta, classif, set_name='train').labels\n",
    "    if isinstance(X_train, list):\n",
    "        X_train = np.array(X_train)\n",
    "    #Print shapes to be sure that dimensions are the same\n",
    "    print(f'The shape of the training data is {X_train.shape} and its labels are {y_train.shape}')\n",
    "    \n",
    "    #We read the validation data\n",
    "    X_val = open_pickle(str(repo_path) + f'/data/features/{f_type}/{classif}_val_{f_type}_{f_name}_fv.p')\n",
    "    y_val = path_label(meta, classif, set_name='val').labels\n",
    "    if isinstance(X_val, list):\n",
    "        X_val = np.array(X_val)\n",
    "    print(f'The shape of the valdiation data is {X_val.shape} and its labels are {y_val.shape}')\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Train_Val_Data(classif, c_name, t_name):\n",
    "    #Color data load\n",
    "    color_space = 'RGB'; f_name = 'ColorStats' #To locate feature matrices\n",
    "    c = color_data_load(classif, color_space, c_name) #Get data\n",
    "    #Texture data load\n",
    "    f_name = '27'\n",
    "    if t_name==None:\n",
    "        t = None\n",
    "        return c\n",
    "    else:\n",
    "        t = texture_data_load(classif, t_name)\n",
    "\n",
    "    #Stack together\n",
    "    X_train = np.hstack((c[0], t[0]))\n",
    "    X_val = np.hstack((c[1], t[1]))\n",
    "    y_train = c[2]\n",
    "    y_val = c[3]\n",
    "    print(f'The shape of the training data is {X_train.shape} and its labels are {y_train.shape}')\n",
    "    print(f'The shape of the valdiation data is {X_val.shape} and its labels are {y_val.shape}')\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training data is (15195, 54) and its labels are (15195,)\n",
      "The shape of the valdiation data is (3796, 54) and its labels are (3796,)\n",
      "The shape of the training data is (15195, 27) and its labels are (15195,)\n",
      "The shape of the valdiation data is (3796, 27) and its labels are (3796,)\n",
      "The shape of the training data is (15195, 81) and its labels are (15195,)\n",
      "The shape of the valdiation data is (3796, 81) and its labels are (3796,)\n"
     ]
    }
   ],
   "source": [
    "classif = 'binary'\n",
    "c_name = 'ColorStats'\n",
    "t_name = '27'\n",
    "X_train, X_val, y_train, y_val = get_Train_Val_Data(classif, c_name, t_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV 1/5] END ......classifier__n_estimators=100;, score=0.855 total time=   5.1s\n",
      "[CV 2/5] END ......classifier__n_estimators=100;, score=0.832 total time=   5.2s\n",
      "[CV 3/5] END ......classifier__n_estimators=100;, score=0.775 total time=   5.2s\n",
      "[CV 4/5] END ......classifier__n_estimators=100;, score=0.735 total time=   5.3s\n",
      "[CV 5/5] END ......classifier__n_estimators=100;, score=0.766 total time=   5.4s\n",
      "[CV 1/5] END ......classifier__n_estimators=200;, score=0.860 total time=  11.1s\n",
      "[CV 2/5] END ......classifier__n_estimators=200;, score=0.833 total time=  11.2s\n",
      "[CV 3/5] END ......classifier__n_estimators=200;, score=0.776 total time=  11.1s\n",
      "[CV 4/5] END ......classifier__n_estimators=200;, score=0.735 total time=  11.2s\n",
      "[CV 5/5] END ......classifier__n_estimators=200;, score=0.771 total time=  10.9s\n",
      "[CV 1/5] END ......classifier__n_estimators=400;, score=0.862 total time=  21.5s\n",
      "[CV 2/5] END ......classifier__n_estimators=400;, score=0.838 total time=  21.2s\n",
      "[CV 3/5] END ......classifier__n_estimators=400;, score=0.780 total time=  21.3s\n",
      "[CV 4/5] END ......classifier__n_estimators=400;, score=0.732 total time=  21.3s\n",
      "[CV 5/5] END ......classifier__n_estimators=400;, score=0.767 total time=  21.1s\n",
      "0.7955742887249737\n"
     ]
    }
   ],
   "source": [
    "grid, pipe = classifier('RF', grid_type='random', scaler=StandardScaler(), verbose=4)\n",
    "model = grid.fit(X_train,y_train)\n",
    "save_pickle(model, str(repo_path) + f'/data/models/{classif}_{pipe.steps[-1][-1]}_{c_name}_{t_name}.p') #Save model\n",
    "y_pred = model.predict(X_val) #predict\n",
    "acc = np.mean(y_pred == y_val) #accuracy\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c1f01218bbaf8a302f18173488403fcc9591627716b9a07a59bd925307e4c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
